from dotenv import load_dotenv
import os
from openai import OpenAI
from create_and_store_vector_embeddings import retriver

load_dotenv()

api_key = os.getenv("GEMINI_API_KEY")
client = OpenAI(
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
    api_key=api_key
)

"""
    Step 0 - Create and store vector embeddings of pdf document -  which is already done
    Step 1 - User gives prompt 
    Step 2 - Create System prompt, and generate multiple user prompts
    Step 3 - Create vector embeddings of each query and perform similarity search with vector database.
    Step 4 - Filter out all unique chunks(Remove duplicates)
    Step 5 - Use relevant chunks to generate response for original user query
"""

def generate_different_user_prompt(user_input, num_variants=3):
    SYSTEM_PROMPT = f"""
    You are an helpfull AI Assistant that rewrites user input queries in different forms for the better document retrieval

    Original Query: "{user_input}"

    Rewrite this query in {num_variants} different ways:

    If the answer is not found in the context, reply with "I don't know based on the document.
    """

    different_qurey = []
    response = client.chat.completions.create(
        model="gemini-2.0-flash",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_input}
        ]
    )

    different_qurey.append(response.choices[0].message.content)
    for query in different_qurey:
        print(query)
    # Split the first string into lines so we did different_query[0], remove 1234567890, and skip empty lines so we did line.strip()
    cleaned_questions = [line.strip("1234567890.") for line in different_qurey[0].split('\n') if line.strip()]

    return cleaned_questions


def get_similar_chunks_from_document(user_input):
    ai_prompts = generate_different_user_prompt(user_input)
    relevant_chunks_search = []
    for index, prompt in enumerate(ai_prompts):
        relevant_chunks = retriver.similarity_search(
            query=prompt
        )
        relevant_chunks_search.append(relevant_chunks)
    return relevant_chunks_search



def parallel_query_retrieval():
    while True:
        user_input = input(">> ")
        if user_input.lower() in ["exit", "quit"]:
            break
        similar_chunks = get_similar_chunks_from_document(user_input)
        seen_contents = set()
        filter_unique_chunks = []
        for chunk in similar_chunks: # can be nested list
            for docs in chunk:
                content = docs.page_content
                if content not in seen_contents:
                    seen_contents.add(content)
                    filter_unique_chunks.append(chunk) # Only adds the actual chunk (the full object with metadata, etc.) to the final list if it's the first time we see that content.

        SYSTEM_PROMPT = f"""
        You are an helpfull AI Assistant who responds base on the available context
        If the answer is not found in the context, reply with "I don't know based on the document.

        Context:
        {seen_contents}
        """

        response = client.chat.completions.create(
            model="gemini-2.0-flash",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_input}
            ]
        )
        
        print("-----> ", response.choices[0].message.content)


parallel_query_retrieval()